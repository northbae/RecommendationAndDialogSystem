import re
from dataclasses import dataclass
from typing import Optional, Dict, Any, List, Tuple
import Levenshtein

from .linguistic_variable import LengthTerm, ImportanceTerm


@dataclass
class ParsedQuery:
    intent: str
    filters: Dict[str, Any]
    exclusions: Dict[str, Any]
    original_query: str
    domain_entity: Optional[str] = None
    target_id: Optional[int] = None
    sentiment: Optional[str] = None


class HybridParser:
    """
    –ü–∞—Ä—Å–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∞–≤–∏–ª –∏ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω–∞.
    """

    def __init__(self):
        print("üí° –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ (–õ–µ–≤–µ–Ω—à—Ç–µ–π–Ω)...")
        self._init_knowledge_base()
        self._init_regex()
        print("‚úÖ –ü–∞—Ä—Å–µ—Ä –≥–æ—Ç–æ–≤!")

    def _init_knowledge_base(self):
        """–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏ —Ñ—Ä–∞–∑ –¥–ª—è –ø–æ–∏—Å–∫–∞."""
        self.knowledge_base: List[Tuple[str, Any, List[str]]] = [
            # --- –ò–ù–¢–ï–ù–¢–´ ---
            ("intent", "help", ["–ø–æ–º–æ—â—å", "—Å–ø—Ä–∞–≤–∫–∞", "—á—Ç–æ —É–º–µ–µ—à—å", "–∫–æ–º–∞–Ω–¥—ã", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è"]),
            ("intent", "help_domain", ["–∫–∞—Ç–µ–≥–æ—Ä–∏–∏", "—Ä—É–±—Ä–∏–∫–∏", "–∞–≤—Ç–æ—Ä—ã", "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞", "—Ç–µ–º—ã", "—Ä–∞–∑–¥–µ–ª—ã"]),
            ("intent", "help_examples", ["–ø—Ä–∏–º–µ—Ä", "–æ–±—Ä–∞–∑–µ—Ü", "–∫–∞–∫ —Å–ø—Ä–æ—Å–∏—Ç—å"]),
            ("intent", "undo", ["–Ω–∞–∑–∞–¥", "–æ—Ç–º–µ–Ω–∏", "–≤–µ—Ä–Ω–∏", "–æ—Ç–∫–∞—Ç–∏—Ç—å", "–ø—Ä–µ–¥—ã–¥—É—â–∏–π"]),
            ("intent", "reset", ["—Å–±—Ä–æ—Å", "—Å–Ω–∞—á–∞–ª–∞", "–∑–∞–Ω–æ–≤–æ", "–æ—á–∏—Å—Ç–∏"]),
            ("intent", "recommend_personal", ["–ø–æ–¥–æ–π–¥–µ—Ç", "–¥–ª—è –º–µ–Ω—è", "–Ω–∞ –º–æ–π –≤–∫—É—Å", "–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ"]),
            ("intent", "recommend_similar", ["–ø–æ—Ö–æ–∂–∏–µ", "–ø–æ–¥–æ–±–Ω—ã–µ", "–∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ"]),
            ("intent", "search", ["–Ω–∞–π–¥–∏", "–ø–æ–∫–∞–∂–∏", "–ø–æ–∏—â–∏", "–≤—ã–≤–µ–¥–∏", "—Ö–æ—á—É", "—Å—Ç–∞—Ç—å–∏", "–Ω–æ–≤–æ—Å—Ç–∏"]),

            # --- –§–ò–õ–¨–¢–†–´ ---
            ("length", LengthTerm.VERY_SHORT, ["–æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ", "–º–∏–∫—Ä–æ", "–∫—Ä–æ—à–µ—á–Ω—ã–µ"]),
            ("length", LengthTerm.SHORT, ["–∫–æ—Ä–æ—Ç–∫–∏–µ", "–Ω–µ–±–æ–ª—å—à–∏–µ", "–º–∞–ª–µ–Ω—å–∫–∏–µ", "–±—ã—Å—Ç—Ä—ã–µ"]),
            ("length", LengthTerm.MEDIUM, ["—Å—Ä–µ–¥–Ω–∏–µ", "–æ–±—ã—á–Ω—ã–µ", "–Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ"]),
            ("length", LengthTerm.LONG, ["–¥–ª–∏–Ω–Ω—ã–µ", "–±–æ–ª—å—à–∏–µ", "–ø–æ–¥—Ä–æ–±–Ω—ã–µ", "–º–µ–¥–ª–µ–Ω–Ω—ã–µ"]),
            ("length", LengthTerm.VERY_LONG, ["–æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ", "–æ–≥—Ä–æ–º–Ω—ã–µ", "–ª–æ–Ω–≥—Ä–∏–¥—ã"]),

            ("importance", ImportanceTerm.HIGH, ["–≤–∞–∂–Ω—ã–µ", "–≥–ª–∞–≤–Ω—ã–µ", "—Ç–æ–ø"]),

            ("date", "DATE_TODAY", ["—Å–µ–≥–æ–¥–Ω—è", "–∑–∞ –¥–µ–Ω—å", "—Å–≤–µ–∂–∏–µ"]),
            ("date", "DATE_YESTERDAY", ["–≤—á–µ—Ä–∞"]),
            ("date", "DATE_WEEK", ["–∑–∞ –Ω–µ–¥–µ–ª—é", "–Ω–µ–¥–µ–ª—å–Ω—ã–µ"]),
            ("date", "DATE_MONTH", ["–∑–∞ –º–µ—Å—è—Ü", "–º–µ—Å—è—á–Ω—ã–µ"]),

            ("media", "MEDIA_VIDEO", ["—Å –≤–∏–¥–µ–æ", "–≤–∏–¥–µ–æ—Ä–æ–ª–∏–∫–æ–º"]),
            ("media", "MEDIA_IMAGE", ["—Å –∫–∞—Ä—Ç–∏–Ω–∫–∞–º–∏", "—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏—è–º–∏", "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏"]),

            ("category", "–°–ø–æ—Ä—Ç", ["—Å–ø–æ—Ä—Ç", "—Ñ—É—Ç–±–æ–ª", "—Ö–æ–∫–∫–µ–π"]),
            ("category", "–≠–∫–æ–Ω–æ–º–∏–∫–∞", ["—ç–∫–æ–Ω–æ–º–∏–∫–∞", "–±–∏–∑–Ω–µ—Å", "—Ñ–∏–Ω–∞–Ω—Å—ã"]),
            ("category", "–ü–æ–ª–∏—Ç–∏–∫–∞", ["–ø–æ–ª–∏—Ç–∏–∫–∞"]),
            ("category", "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", ["—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏", "it", "–∏–∏"]),
            ("category", "–û–±—â–µ—Å—Ç–≤–æ", ["–æ–±—â–µ—Å—Ç–≤–æ", "–∫—É–ª—å—Ç—É—Ä–∞", "–∏—Å–∫—É—Å—Å—Ç–≤–æ"]),

            ("author", "–ò–≤–∞–Ω–æ–≤ –ü–µ—Ç—Ä", ["–∏–≤–∞–Ω–æ–≤–∞", "–∏–≤–∞–Ω–æ–≤"]),
            ("author", "–ú–æ—Ä–æ–∑–æ–≤ –ê–Ω–¥—Ä–µ–π", ["–º–æ—Ä–æ–∑–æ–≤–∞", "–º–æ—Ä–æ–∑–æ–≤"]),
        ]

    def _init_regex(self):
        """Regex —Ç–æ–ª—å–∫–æ –¥–ª—è —Å–ª—É–∂–µ–±–Ω—ã—Ö —Ü–µ–ª–µ–π"""
        self.like_pattern = re.compile(r'\b(–Ω—Ä–∞–≤–∏—Ç—Å—è|–ø–æ–Ω—Ä–∞–≤–∏–ª–∞—Å—å|—Ö–æ—Ä–æ—à–∞—è|—Å—É–ø–µ—Ä)\b', re.IGNORECASE)
        self.dislike_pattern = re.compile(r'\b(–Ω–µ\s+–Ω—Ä–∞–≤–∏—Ç—Å—è|–Ω–µ\s+–ø–æ–Ω—Ä–∞–≤–∏–ª–∞—Å—å|–ø–ª–æ—Ö–∞—è)\b', re.IGNORECASE)
        self.exclusion_pattern = re.compile(r'^\s*(–Ω–µ|–∫—Ä–æ–º–µ|–±–µ–∑|–∏—Å–∫–ª—é—á–∞—è)\s*$', re.IGNORECASE)
        self.id_pattern = re.compile(r'(?:—Å—Ç–∞—Ç—å[—é–∏–µ]|‚Ññ)?\s*(\d+)', re.IGNORECASE)
        self.domain_question_pattern = re.compile(r'^\s*(–∫–∞–∫–∏–µ|–∫—Ç–æ|—á—Ç–æ|—Å–ø–∏—Å–æ–∫|–ø–µ—Ä–µ—á–∏—Å–ª–∏)\s+', re.IGNORECASE)
        self.unclear_pattern = re.compile(r'^(?:–Ω—É\s|—ç—ç—ç|^\s*$)', re.IGNORECASE)
        self.offensive_pattern = re.compile(r'\b(—Ç—É–ø–æ–π|–¥—É—Ä–∞–∫|–∏–¥–∏–æ—Ç)\b', re.IGNORECASE)
        self.similar_pattern = re.compile(r'\b(–ø–æ—Ö–æ–∂–∏–µ|–ø–æ–¥–æ–±–Ω—ã–µ|–∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–µ)\b', re.IGNORECASE)

    def _find_closest_match(self, token: str) -> Optional[Tuple[str, Any]]:
        """–ò—â–µ—Ç –±–ª–∏–∂–∞–π—à–µ–µ —Å–ª–æ–≤–æ/—Ñ—Ä–∞–∑—É –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"""
        best_match = None
        max_dist = 1 if len(token) <= 4 else 2
        min_dist = max_dist + 1

        for f_type, f_val, keywords in self.knowledge_base:
            for keyword in keywords:
                dist = Levenshtein.distance(token, keyword)
                if dist < min_dist:
                    min_dist = dist
                    best_match = (f_type, f_val)
        return best_match if best_match and min_dist <= max_dist else None

    def parse(self, query: str) -> ParsedQuery:
        query = query.strip().lower()

        # 1. –ë—ã—Å—Ç—Ä—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
        if self.unclear_pattern.match(query) or len(query) < 3: return ParsedQuery("unclear", {}, {}, query)
        if self.offensive_pattern.search(query): return ParsedQuery("offensive", {}, {}, query)

        # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –∏ sentiment
        target_id_match = self.id_pattern.search(query)
        target_id = int(target_id_match.group(1)) if target_id_match else None
        sentiment = "dislike" if self.dislike_pattern.search(query) else (
            "like" if self.like_pattern.search(query) else None)

        if sentiment and target_id:
            return ParsedQuery("state_change", {}, {}, query, target_id=target_id, sentiment=sentiment)

        # 3. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –¥–æ–º–µ–Ω–µ
        if self.domain_question_pattern.match(query):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤ —ç—Ç–æ–º –≤–æ–ø—Ä–æ—Å–µ —Ñ–∏–ª—å—Ç—Ä. –ï—Å–ª–∏ –¥–∞ - —ç—Ç–æ –ø–æ–∏—Å–∫.
            temp_filters, _ = self._extract_filters(query)
            if not temp_filters:  # –ï—Å–ª–∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –ù–ï–¢, —Ç–æ —ç—Ç–æ –≤–æ–ø—Ä–æ—Å –æ –¥–æ–º–µ–Ω–µ
                domain_entity = "–∞–≤—Ç–æ—Ä—ã" if "–∞–≤—Ç–æ—Ä" in query or "–ø–∏—à–µ—Ç" in query else "–∫–∞—Ç–µ–≥–æ—Ä–∏–∏"
                return ParsedQuery("help_domain", {}, {}, query, domain_entity=domain_entity)

        # 4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏ –∏–Ω—Ç–µ–Ω—Ç–æ–≤
        filters, exclusions, intents = self._extract_all(query)

        # 5. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö
        if self.similar_pattern.search(query) and target_id:
            return ParsedQuery("recommend_similar", {}, {}, query, target_id=target_id)

        # 6. –§–ò–ù–ê–õ–¨–ù–û–ï –†–ï–®–ï–ù–ò–ï
        if filters or exclusions:
            return ParsedQuery("search", filters, exclusions, query, 1.0)

        if intents:
            best_intent = max(intents, key=intents.get)
            return ParsedQuery(best_intent, {}, {}, query, 0.9, target_id=target_id)

        return ParsedQuery("unknown", {}, {}, query, 0.0)

    def _extract_all(self, query: str) -> Tuple[Dict, Dict, Dict]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞: —Ñ–∏–ª—å—Ç—Ä—ã, –∏—Å–∫–ª—é—á–µ–Ω–∏—è –∏ –∏–Ω—Ç–µ–Ω—Ç—ã"""
        words = query.split()
        tokens = words + [" ".join(words[i:i + 2]) for i in range(len(words) - 1)]

        intents = {}
        filters = {}
        exclusions = {}
        is_exclusion_zone = False

        for token in set(tokens):
            if self.exclusion_pattern.match(token):
                is_exclusion_zone = True
                continue

            match = self._find_closest_match(token)

            if match:
                f_type, f_val = match
                target_dict = exclusions if is_exclusion_zone else filters

                if f_type == "intent":
                    intents[f_val] = intents.get(f_val, 0) + 1
                else:
                    # --- –ì–õ–ê–í–ù–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï ---
                    if f_type in ["category", "media"]:
                        # –í—Å–µ–≥–¥–∞ —Å–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫
                        target_dict.setdefault(f_type, []).append(f_val)
                    else:
                        target_dict[f_type] = f_val

        return filters, exclusions, intents

    def _extract_filters(self, query: str) -> Tuple[Dict, Dict]:
        """–ë—ã—Å—Ç—Ä–æ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ"""
        filters, exclusions, _ = self._extract_all(query)
        return filters, exclusions